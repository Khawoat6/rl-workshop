{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 1: Discrete States and Discrete Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridworld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld:\n",
    "    def __init__(self, sz = (3,3), start = (0,0), goal = (0,2), traps = [(0,1)],\n",
    "                 goal_reward = 5, trap_reward = -3, move_reward = -1, wind_p = 0.3):\n",
    "        self.sz = sz\n",
    "        self.action_space = ['U','L','D','R']\n",
    "        #create grids\n",
    "        self.grid_keys = [(i,j) for i in range(sz[0]) for j in range(sz[1])]\n",
    "        self.start =start\n",
    "        self.goal = goal\n",
    "        self.traps = traps\n",
    "        self.move_reward = move_reward\n",
    "        self.trap_reward = trap_reward\n",
    "        self.goal_reward = goal_reward\n",
    "        self.wind_p = wind_p\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.traversed = [self.start]\n",
    "        self.i = self.start[0]\n",
    "        self.j = self.start[1]\n",
    "        self.done = False\n",
    "        #physical grid\n",
    "        self.physical_grid = dict.fromkeys(self.grid_keys,['F','x'])\n",
    "        self.physical_grid[self.start] = ['F','o']\n",
    "        self.physical_grid[self.goal] = ['G','x']\n",
    "        for t in self.traps: self.physical_grid[t] = ['T','x']\n",
    "        #reward grid\n",
    "        self.reward_grid = dict.fromkeys(self.grid_keys,0)\n",
    "        self.reward_grid[self.goal] = self.goal_reward\n",
    "        for t in self.traps: self.reward_grid[t] = self.trap_reward\n",
    "        return((self.i,self.j))\n",
    "    def print_reward(self,visible_only=False):\n",
    "        for i in range(self.sz[0]):\n",
    "            print('\\n----------')\n",
    "            for j in range(self.sz[1]):\n",
    "                if visible_only:\n",
    "                    out = self.reward_grid[(i,j)] if (i,j) in self.traversed else 'NA'\n",
    "                else:\n",
    "                    out = self.reward_grid[(i,j)]\n",
    "                print(f'{out} |',end='')\n",
    "    def print_physical(self,visible_only=False):\n",
    "        for i in range(self.sz[0]):\n",
    "            print('\\n------------------------------------')\n",
    "            for j in range(self.sz[1]):\n",
    "                if visible_only:\n",
    "                    out = self.physical_grid[(i,j)] if (i,j) in self.traversed else ['NA','NA']\n",
    "                else:\n",
    "                    out = self.physical_grid[(i,j)]\n",
    "                print(f'{out} |',end='')\n",
    "    def update_physical(self):\n",
    "        for key in self.grid_keys:\n",
    "            self.physical_grid[key][1] = 'x'\n",
    "        tile = self.physical_grid[(self.i,self.j)][0] \n",
    "        self.physical_grid[(self.i,self.j)] = [tile,'o']\n",
    "    def wind(self):\n",
    "        if np.random.uniform() < self.wind_p:\n",
    "            pos = self.i - 1\n",
    "            self.i = pos if pos >= 0 else 0\n",
    "    def step(self,action_idx):\n",
    "        reward = self.move_reward\n",
    "        i,j = self.i,self.j\n",
    "        action = self.action_space[action_idx]\n",
    "        if action == 'U':\n",
    "            i -= 1\n",
    "        elif action == 'L':\n",
    "            j -= 1\n",
    "        elif action == 'D':\n",
    "            i += 1\n",
    "        elif action == 'R':\n",
    "            j += 1\n",
    "        #check legality\n",
    "        if (i,j) in self.grid_keys:\n",
    "            #update position\n",
    "            self.i,self.j = i,j\n",
    "            #wind blows\n",
    "            self.wind()\n",
    "            #save traversed\n",
    "            self.traversed.append((self.i,self.j))\n",
    "            #update physical\n",
    "            self.update_physical()\n",
    "            #update reward\n",
    "            reward += self.reward_grid[(self.i,self.j)]\n",
    "        else:\n",
    "            pass\n",
    "        if (self.i,self.j) == self.goal: self.done = True\n",
    "        #return s',r, done or not\n",
    "        return((self.i,self.j),reward,self.done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------\n",
      "['F', 'o'] |['T', 'x'] |['G', 'x'] |\n",
      "------------------------------------\n",
      "['F', 'x'] |['F', 'x'] |['F', 'x'] |\n",
      "------------------------------------\n",
      "['F', 'x'] |['F', 'x'] |['F', 'x'] |\n",
      "----------\n",
      "0 |-3 |5 |\n",
      "----------\n",
      "0 |0 |0 |\n",
      "----------\n",
      "0 |0 |0 |"
     ]
    }
   ],
   "source": [
    "g = Gridworld()\n",
    "g.print_physical(visible_only=False)\n",
    "g.print_reward(visible_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0, 1), -4, False)\n",
      "((0, 2), 4, True)\n",
      "\n",
      "------------------------------------\n",
      "['F', 'x'] |['T', 'x'] |['G', 'o'] |\n",
      "------------------------------------\n",
      "['NA', 'NA'] |['NA', 'NA'] |['NA', 'NA'] |\n",
      "------------------------------------\n",
      "['NA', 'NA'] |['NA', 'NA'] |['NA', 'NA'] |"
     ]
    }
   ],
   "source": [
    "print(g.step(3))\n",
    "print(g.step(3))\n",
    "g.print_physical(visible_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, policy, gamma = 1, \n",
    "                 start_epsilon = 0.9, end_epsilon = 0.1, epsilon_decay = 0.9):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.gamma = gamma\n",
    "        self.v = dict.fromkeys(self.env.grid_keys,0)\n",
    "        self.q = defaultdict(lambda: np.zeros(len(self.env.action_space)))\n",
    "        self.start_epsilon = start_epsilon\n",
    "        self.end_epsilon = end_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "    def get_epsilon(self,n_episode):\n",
    "        epsilon = max(self.start_epsilon * (self.epsilon_decay**n_episode),self.end_epsilon)\n",
    "        return(epsilon)\n",
    "    def select_action(self,state,epsilon):\n",
    "        if np.random.uniform() < epsilon:\n",
    "            action = np.random.choice(range(len(self.env.action_space)))\n",
    "        else:\n",
    "            action = self.policy[state]\n",
    "        return(action)\n",
    "    def print_policy(self):\n",
    "        for i in range(self.env.sz[0]):\n",
    "            print('\\n----------')\n",
    "            for j in range(self.env.sz[1]):\n",
    "                p=self.policy[(i,j)]\n",
    "                out = self.env.action_space[p]\n",
    "                print(f'{out} |',end='')\n",
    "    def run_episode(self, n_episode):\n",
    "        state = self.env.reset()\n",
    "        while True:\n",
    "            epsilon = self.get_epsilon(n_episode)\n",
    "            old_state = state\n",
    "            action = self.select_action(state,epsilon)\n",
    "            state,reward,done = self.env.step(action)\n",
    "            print(old_state,state,reward,done)\n",
    "            if done: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------\n",
      "D |R |U |\n",
      "----------\n",
      "R |R |U |\n",
      "----------\n",
      "R |U |U |"
     ]
    }
   ],
   "source": [
    "env = Gridworld()\n",
    "policy = {(0, 0): 2,\n",
    "          (0, 1): 3,\n",
    "          (0, 2): 0,\n",
    "          (1, 0): 3,\n",
    "          (1, 1): 3,\n",
    "          (1, 2): 0,\n",
    "          (2, 0): 3,\n",
    "          (2, 1): 0,\n",
    "          (2, 2): 0}\n",
    "a = Agent(env,policy,gamma=1)\n",
    "a.print_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) (0, 0) -1 False\n",
      "(0, 0) (1, 0) -1 False\n",
      "(1, 0) (0, 1) -4 False\n",
      "(0, 1) (0, 2) 4 True\n"
     ]
    }
   ],
   "source": [
    "a.run_episode(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Problem\n",
    "* Evaluate deterministic policies in a deterministic environment\n",
    "* Evaluate deterministic policies in a stochastic environment\n",
    "* Evaluate stochastic policies in a stochastic environment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Problem\n",
    "* Monte Carlo\n",
    "* First-visit\n",
    "* Greedy in the Limit with Infinite Exploration\n",
    "* GLIE with constant learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
