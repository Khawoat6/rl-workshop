{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 1: Discrete States and Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "#complete these scripts during session / as homework\n",
    "# from answers.environments import Gridworld\n",
    "# from answers.agents import MCAgent\n",
    "#cross check when you finish\n",
    "from solutions.environments import Gridworld\n",
    "from solutions.agents import MCAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment - Gridworld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What The Gridworld Looks Like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------\n",
      "['F', 'o'] |['T', 'x'] |['G', 'x'] |\n",
      "------------------------------------\n",
      "['F', 'x'] |['F', 'x'] |['F', 'x'] |\n",
      "------------------------------------\n",
      "['F', 'x'] |['F', 'x'] |['F', 'x'] |"
     ]
    }
   ],
   "source": [
    "env = Gridworld()\n",
    "env.print_physical(visible_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3]), array(['U', 'L', 'D', 'R'], dtype='<U1'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space, env.action_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------\n",
      "0 |-3 |5 |\n",
      "----------\n",
      "0 |0 |0 |\n",
      "----------\n",
      "0 |0 |0 |"
     ]
    }
   ],
   "source": [
    "env.print_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.move_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What The Agent Usually Sees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------\n",
      "['F', 'o'] |['NA', 'NA'] |['NA', 'NA'] |\n",
      "------------------------------------\n",
      "['NA', 'NA'] |['NA', 'NA'] |['NA', 'NA'] |\n",
      "------------------------------------\n",
      "['NA', 'NA'] |['NA', 'NA'] |['NA', 'NA'] |"
     ]
    }
   ],
   "source": [
    "env.print_physical(visible_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1, 0), -1, False)\n",
      "((1, 1), -1, False)\n",
      "\n",
      "------------------------------------\n",
      "['F', 'x'] |['NA', 'NA'] |['NA', 'NA'] |\n",
      "------------------------------------\n",
      "['F', 'x'] |['F', 'o'] |['NA', 'NA'] |\n",
      "------------------------------------\n",
      "['NA', 'NA'] |['NA', 'NA'] |['NA', 'NA'] |"
     ]
    }
   ],
   "source": [
    "#go down\n",
    "action = np.argwhere(env.action_text=='D')\n",
    "print(env.step(action))\n",
    "#then right\n",
    "action = np.argwhere(env.action_text=='R')\n",
    "print(env.step(action))\n",
    "env.print_physical(visible_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deterministic Environment vs Stochastic Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1, 0), -1, False)\n",
      "((0, 1), -4, False)\n",
      "\n",
      "------------------------------------\n",
      "['F', 'x'] |['T', 'o'] |['NA', 'NA'] |\n",
      "------------------------------------\n",
      "['F', 'x'] |['NA', 'NA'] |['NA', 'NA'] |\n",
      "------------------------------------\n",
      "['NA', 'NA'] |['NA', 'NA'] |['NA', 'NA'] |"
     ]
    }
   ],
   "source": [
    "env = Gridworld(wind_p=0.5)\n",
    "#go down\n",
    "action = np.argwhere(env.action_text=='D')\n",
    "print(env.step(action))\n",
    "#then right\n",
    "action = np.argwhere(env.action_text=='R')\n",
    "print(env.step(action))\n",
    "env.print_physical(visible_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deterministic Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Policy - Epsilon Greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (State-)Action Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal State and Action Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Reinforcement Learning Problems - Monte Carlo Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What are some other ways of solving reinforcement learning problems? How are they better or worse than Monte Carlo methods e.g. performance, data requirements, etc.?\n",
    "* Play around with Gridworld. Tweak these variables and see what happens:\n",
    "    * Wing probability\n",
    "    * Move rewards\n",
    "    * Discount factor\n",
    "    * Epsilon and how to decay it (or not)\n",
    "* Solve at least one of the following OpenAI gym environments with discrete states and actions:\n",
    "    * FrozenLake-v0\n",
    "    * Taxi-v2\n",
    "    * Blackjack-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Gridworld' object has no attribute 'grid_keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-be3693e821bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridworld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwind_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_keys\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Gridworld' object has no attribute 'grid_keys'"
     ]
    }
   ],
   "source": [
    "env = Gridworld(wind_p=0.1)\n",
    "policy = dict.fromkeys(env.grid_keys)\n",
    "for key in env.grid_keys: policy[key] = np.random.choice(np.arange(len(env.action_space)))\n",
    "a = Agent(env,policy)\n",
    "a.print_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.mc_control_glie(lr=0.01)\n",
    "a.print_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Problem\n",
    "* Evaluate deterministic policies and environments\n",
    "* Evaluate stochastic policies and environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate deterministic policies and environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deterministic env\n",
    "env = Gridworld(wind_p=0.)\n",
    "#deterministic policy\n",
    "policy_a = {(0, 0): 3,\n",
    "          (0, 1): 3,\n",
    "          (0, 2): 0,\n",
    "          (1, 0): 3,\n",
    "          (1, 1): 3,\n",
    "          (1, 2): 0,\n",
    "          (2, 0): 3,\n",
    "          (2, 1): 0,\n",
    "          (2, 2): 0}\n",
    "policy_b = {(0, 0): 2,\n",
    "          (0, 1): 3,\n",
    "          (0, 2): 0,\n",
    "          (1, 0): 2,\n",
    "          (1, 1): 2,\n",
    "          (1, 2): 0,\n",
    "          (2, 0): 3,\n",
    "          (2, 1): 3,\n",
    "          (2, 2): 0}\n",
    "\n",
    "#peek\n",
    "env.print_reward()\n",
    "env.print_physical()\n",
    "print('\\n')\n",
    "a = Agent(env,policy_a,gamma=0.9)\n",
    "print('Policy a')\n",
    "a.print_policy()\n",
    "print('\\n')\n",
    "print('Policy b')\n",
    "a.policy = policy_b\n",
    "a.print_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in env.grid_keys:\n",
    "    a.v[state] = a.get_v(state)\n",
    "a.print_v()\n",
    "a.print_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in env.grid_keys:\n",
    "    for action in range(len(env.action_space)):\n",
    "        a.q[state][action] = a.get_q(state,action)\n",
    "a.q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate stochastic policies and environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Problem\n",
    "* Monte Carlo\n",
    "* First-visit\n",
    "\n",
    "Bonus\n",
    "* Greedy in the Limit with Infinite Exploration\n",
    "* GLIE with constant learning rate\n",
    "\n",
    "Homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
