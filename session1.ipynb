{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 1: Markov Decision Processes and Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What and Why of Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Level of workflow automation in classes of machine learning algorithm\n",
    "* Supervised Learning\n",
    "$$\\text{Raw Data (X,y)} \\Rightarrow \\text{Labeled Data} \\rightarrow \\text{Model} \\rightarrow \\text{Predicted Labels} \\Rightarrow \\text{Actions}$$\n",
    "\n",
    "* Unsupervised Learning\n",
    "$$\\text{Raw Data (X)} \\rightarrow \\text{Model} \\rightarrow \\text{Predicted Clusters} \\Rightarrow \\text{Labled Clusters} \\Rightarrow \\text{Actions}$$\n",
    "\n",
    "* Reinforcement Learning\n",
    "$$\\text{Raw Data} \\Rightarrow \\text{RL Scheme (S,A,R,S',A')} \\rightarrow \\text{Model} \\rightarrow \\text{Action}$$\n",
    "\n",
    "\n",
    "where $\\Rightarrow$ denotes transformations that require humans and $\\rightarrow$ denotes those that do not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RL Framework](img/rl_framework.png)\n",
    "Source: [Sutton and Barto](https://cdn.preterhuman.net/texts/science_and_technology/artificial_intelligence/Reinforcement%20Learning%20%20An%20Introduction%20-%20Richard%20S.%20Sutton%20,%20Andrew%20G.%20Barto.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few reasons [Deep Reinforcement Learning Doesn't Work Yet](https://www.alexirpan.com/2018/02/14/rl-hard.html):\n",
    "* It still takes too much time to learn simple things; for instance, one of the previous state-of-the-art Rainbow model takes over 80 hours to learn to play Atari games at human level. Even a child would only need a few hours on their phone.\n",
    "* Supervised learning works so well.\n",
    "* Instead of labels, you decide on a reward to motivate the models and it is a tricky business; for example, how would give rewards to an humanoid robot agent for it to learn how to walk?\n",
    "* There are so many hyperparameters to take care of.\n",
    "* A lot of times it is [just random search](https://arxiv.org/abs/1803.07055)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it has been successful (better than supervised learning) in:\n",
    "* Beating professional [Go players](https://deepmind.com/research/alphago/), [SuperSmash Bro](https://www.youtube.com/watch?v=dXJUlqBsZtE) and, to some extent [DotA players](https://blog.openai.com/openai-five/).\n",
    "* Reduce data center power usage and [save 40% of electricity bill](https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/)\n",
    "* Neural architecture search for better algorithms aka [AutoML](https://cloud.google.com/automl/)\n",
    "* Control tasks for robotics\n",
    "* Simple ads bidding for online marketing\n",
    "\n",
    "And most likely being tried for:\n",
    "* [Algorithmic trading](http://www.wildml.com/2018/02/introduction-to-learning-to-trade-with-reinforcement-learning/)\n",
    "* Supply chain management\n",
    "* Drug discovery\n",
    "* Security\n",
    "* Recommendation engines\n",
    "\n",
    "![Foundational Flaw](img/foundational_flaw.png)\n",
    "\n",
    "Source: [Reinforcement learningâ€™s foundational flaw](https://thegradient.pub/why-rl-is-flawed/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment only if you're running from google colab\n",
    "# !git clone https://github.com/Datatouille/rl-workshop\n",
    "# !mv rl-workshop/* .\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "#cross check with our solutions once you finish\n",
    "from solutions.agents import MCAgent\n",
    "from solutions.environments import Gridworld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill in The Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from collections import defaultdict\n",
    "# import sys\n",
    "\n",
    "# \"\"\"\n",
    "# Coding assignment order:\n",
    "# 1. select_action\n",
    "# 2. get_v\n",
    "# 3. get_q\n",
    "# 4. mc_control_q\n",
    "# 5. mc_control_glie\n",
    "# \"\"\"\n",
    "\n",
    "# class MCAgent:\n",
    "#     def __init__(self, env, policy, gamma = 0.9, \n",
    "#                  start_epsilon = 0.9, end_epsilon = 0.1, epsilon_decay = 0.9):\n",
    "#         self.env = env\n",
    "#         self.n_action = len(self.env.action_space)\n",
    "#         self.policy = policy\n",
    "#         self.gamma = gamma\n",
    "#         self.v = dict.fromkeys(self.env.state_space,0)\n",
    "#         self.n_v = dict.fromkeys(self.env.state_space,0)\n",
    "#         self.q = defaultdict(lambda: np.zeros(self.n_action))\n",
    "#         self.n_q = defaultdict(lambda: np.zeros(self.n_action))\n",
    "#         self.start_epsilon = start_epsilon\n",
    "#         self.end_epsilon = end_epsilon\n",
    "#         self.epsilon_decay = epsilon_decay\n",
    "#     def get_epsilon(self,n_episode):\n",
    "#         epsilon = max(self.start_epsilon * (self.epsilon_decay**n_episode),self.end_epsilon)\n",
    "#         return(epsilon)\n",
    "#     def get_v(self,start_state,epsilon = 0.):\n",
    "#         episode = self.run_episode(start_state,epsilon)\n",
    "#         \"\"\"\n",
    "#         Write the code to calculate the state value function of a state \n",
    "#         given a deterministic policy.\n",
    "#         \"\"\"\n",
    "#         v=0\n",
    "        \n",
    "#         return(v)\n",
    "#     def get_q(self, start_state, first_action, epsilon=0.):\n",
    "#         episode = self.run_episode(start_state,epsilon,first_action)\n",
    "#         \"\"\"\n",
    "#         Write the code to calculate the action function of a state \n",
    "#         given a deterministic policy.\n",
    "#         \"\"\"\n",
    "#         q=0\n",
    "#         return(q)\n",
    "#     def select_action(self,state,epsilon):\n",
    "#         \"\"\"\n",
    "#         Currently the agent only selects a random action.\n",
    "#         Write the code to make the agent perform \n",
    "#         according to an epsilon-greedy policy.\n",
    "#         \"\"\"\n",
    "#         probs = np.ones(self.n_action) * (1 / self.n_action)\n",
    "#         action = np.random.choice(np.arange(self.n_action),p=probs)\n",
    "#         return(action)\n",
    "#     def print_policy(self):\n",
    "#         for i in range(self.env.sz[0]):\n",
    "#             print('\\n----------')\n",
    "#             for j in range(self.env.sz[1]):\n",
    "#                 p=self.policy[(i,j)]\n",
    "#                 out = self.env.action_text[p]\n",
    "#                 print(f'{out} |',end='')\n",
    "#     def print_v(self, decimal = 1):\n",
    "#         for i in range(self.env.sz[0]):\n",
    "#             print('\\n---------------')\n",
    "#             for j in range(self.env.sz[1]):\n",
    "#                 out=np.round(self.v[(i,j)],decimal)\n",
    "#                 print(f'{out} |',end='')\n",
    "#     def run_episode(self, start, epsilon, first_action = None):\n",
    "#         result = []\n",
    "#         state = self.env.reset(start)\n",
    "#         #dictate first action to iterate q\n",
    "#         if first_action is not None:\n",
    "#             action = first_action\n",
    "#             next_state,reward,done = self.env.step(action)\n",
    "#             result.append((state,action,reward,next_state,done))\n",
    "#             state = next_state\n",
    "#             if done: return(result)\n",
    "#         while True:\n",
    "#             action = self.select_action(state,epsilon)\n",
    "#             next_state,reward,done = self.env.step(action)\n",
    "#             result.append((state,action,reward,next_state,done))\n",
    "#             state = next_state\n",
    "#             if done: break\n",
    "#         return(result)\n",
    "#     def update_policy_q(self):\n",
    "#         for state in self.env.state_space:\n",
    "#             self.policy[state] = np.argmax(self.q[state])\n",
    "#     def mc_predict_v(self,n_episode=10000,first_visit=True):\n",
    "#         for t in range(n_episode):\n",
    "#             traversed = []\n",
    "#             e = self.get_epsilon(t)\n",
    "#             transitions = self.run_episode(self.env.start,e)\n",
    "#             states,actions,rewards,next_states,dones = zip(*transitions)\n",
    "#             for i in range(len(transitions)):\n",
    "#                 if first_visit and (states[i] not in traversed):\n",
    "#                     traversed.append(states[i])\n",
    "#                     self.n_v[states[i]]+=1\n",
    "#                     discounts = np.array([self.gamma**j for j in range(len(transitions)+1)])\n",
    "#                     self.v[states[i]]+= sum(rewards[i:]*discounts[:-(1+i)])\n",
    "#         for state in self.env.state_space:\n",
    "#             if state != self.env.goal:\n",
    "#                 self.v[state] = self.v[state] / self.n_v[state]\n",
    "#             else:\n",
    "#                 self.v[state] = 0\n",
    "    \n",
    "#     def mc_predict_q(self,n_episode=10000,first_visit=True):\n",
    "#         for t in range(n_episode):\n",
    "#             traversed = []\n",
    "#             e = self.get_epsilon(t)\n",
    "#             transitions = self.run_episode(self.env.start,e)\n",
    "#             states,actions,rewards,next_states,dones = zip(*transitions)\n",
    "#             for i in range(len(transitions)):\n",
    "#                 if first_visit and ((states[i],actions[i]) not in traversed):\n",
    "#                     traversed.append((states[i],actions[i]))\n",
    "#                     self.n_q[states[i]][actions[i]]+=1\n",
    "#                     discounts = np.array([self.gamma**j for j in range(len(transitions)+1)])\n",
    "#                     self.q[states[i]][actions[i]]+= sum(rewards[i:]*discounts[:-(1+i)])\n",
    "#                 elif not first_visit:\n",
    "#                     self.n_q[states[i]][actions[i]]+=1\n",
    "#                     discounts = np.array([self.gamma**j for j in range(len(transitions)+1)])\n",
    "#                     self.q[states[i]][actions[i]]+= sum(rewards[i:]*discounts[:-(1+i)])\n",
    "\n",
    "#         #print(self.q,self.n_q)\n",
    "#         for state in self.env.state_space:\n",
    "#             for action in range(self.n_action):\n",
    "#                 if state != self.env.goal:\n",
    "#                     self.q[state][action] = self.q[state][action] / self.n_q[state][action]\n",
    "#                 else:\n",
    "#                     self.q[state][action] = 0\n",
    "        \n",
    "#     def mc_control_q(self,n_episode=10000,first_visit=True):\n",
    "#         \"\"\"\n",
    "#         Write the code to perform Monte Carlo Control\n",
    "#         Hint: You just need to do prediction then update the policy\n",
    "#         \"\"\"\n",
    "#         pass\n",
    "        \n",
    "#     def mc_control_glie(self,n_episode=10000,first_visit=True,lr=0.):\n",
    "#         \"\"\"\n",
    "#         Taking hints from the mc_predict_q and mc_control_q methods, write the code to\n",
    "#         perform GLIE Monte Carlo control.\n",
    "#         \"\"\"\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning formulates interaction between an **agent** and its **environment** as **Markov decision processes**. For a given **state**, an agent takes an **action** based on the current **state**. In response to that action at that state, the agent will then get some **reward** from the environment, and that state changes to the next one.\n",
    "\n",
    "$S_0 \\rightarrow A_0 \\rightarrow R_1 \\rightarrow S_1 \\rightarrow A_1 \\rightarrow R_2 \\rightarrow S_2 \\rightarrow ... \\rightarrow S_{t-1} \\rightarrow A_{t-1} \\rightarrow R_t \\rightarrow S_{t}$\n",
    "\n",
    "where $t$ is the last time step and $S_t$ is the **terminal state** meaning an **episode** of the interactions ended. RL problems that have an end are called **episodic tasks** and those that do not are called **continuous tasks**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RL Framework](img/rl_framework.png)\n",
    "Source: [Sutton and Barto](https://cdn.preterhuman.net/texts/science_and_technology/artificial_intelligence/Reinforcement%20Learning%20%20An%20Introduction%20-%20Richard%20S.%20Sutton%20,%20Andrew%20G.%20Barto.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "* Walking robots\n",
    "    * Environment: sidewalks\n",
    "    * Agent: a robot\n",
    "    * States: positions, velocities and accelerations of body parts\n",
    "    * Actions: move arms, legs, various joints in the body\n",
    "    * Rewards: fall or not\n",
    "    * Episodes: seconds, miliseconds until fall\n",
    "* Ads bidding\n",
    "    * Environment: Google Adwords\n",
    "    * Agent: an ecommerce company\n",
    "    * States: campaign impressions, clicks, purchases\n",
    "    * Actions: adjust bid prices and budget\n",
    "    * Rewards: conversion rates, cost of new customer acquisitions\n",
    "    * Episodes: daily or hourly until campaign ends\n",
    "* Retail stock trading\n",
    "    * Environment: the stock market\n",
    "    * Agent: a retail investor who cannot influence market prices\n",
    "    * States: market prices, volumes, and other indicators\n",
    "    * Actions: buy, hold, sell\n",
    "    * Rewards: returns, returns adjusted by volatility, and so on\n",
    "    * Episodes: daily, hourly, every second until we are extremely rich or broke\n",
    "    \n",
    "**Concept Assigment** Come up with one or more scenarios that you think could be framed as a reinforcement learning problem and list their elements like the example above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Property - One-step Dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most fundamental assumptions of many reinforcement learning algorithms is that the interactions of the world has **Markov property**. That is, the transitions between states only depend on the current state and action, and not the history of transitions formulated as:\n",
    "\n",
    "$$P(s_{t+1},r_{t+1}|s_t,a_t,r_t,s_{t-1},a_{t-1},...,r_1,s_0,a_0) = P(s_{t+1},r_{t+1}|s_t,a_t)$$\n",
    "\n",
    "This can be seen as unrealistic as we intuitively use cues from a history of transitions to choose our actions in real life but as you will learn its simplicity has merits in terms of reduced complexity in real-world applications and we can incorporate long-term goals into reinforcement learning. In fact, long-term goals are one of the most important research area in reinforcement learning today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment - Gridworld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What The Gridworld Looks Like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `F` - Free tiles; agent can move to\n",
    "* `T` - Traps; agent recieves some penalty\n",
    "* `G` - The Goal, episode ends once reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------\n",
      "['F', 'o'] |['T', 'x'] |['G', 'x'] |\n",
      "------------------------------------\n",
      "['F', 'x'] |['F', 'x'] |['F', 'x'] |\n",
      "------------------------------------\n",
      "['F', 'x'] |['F', 'x'] |['F', 'x'] |"
     ]
    }
   ],
   "source": [
    "env = Gridworld()\n",
    "env.print_physical(visible_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3]), array(['U', 'L', 'D', 'R'], dtype='<U1'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space, env.action_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------\n",
      "0 |-5 |5 |\n",
      "----------\n",
      "0 |0 |0 |\n",
      "----------\n",
      "0 |0 |0 |"
     ]
    }
   ],
   "source": [
    "env.print_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.move_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What The Agent Usually Sees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------\n",
      "['F', 'o'] |['NA', 'NA'] |['NA', 'NA'] |\n",
      "------------------------------------\n",
      "['NA', 'NA'] |['NA', 'NA'] |['NA', 'NA'] |\n",
      "------------------------------------\n",
      "['NA', 'NA'] |['NA', 'NA'] |['NA', 'NA'] |"
     ]
    }
   ],
   "source": [
    "env.print_physical(visible_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1, 0), -1, False)\n",
      "((1, 1), -1, False)\n",
      "\n",
      "------------------------------------\n",
      "['F', 'x'] |['NA', 'NA'] |['NA', 'NA'] |\n",
      "------------------------------------\n",
      "['F', 'x'] |['F', 'o'] |['NA', 'NA'] |\n",
      "------------------------------------\n",
      "['NA', 'NA'] |['NA', 'NA'] |['NA', 'NA'] |"
     ]
    }
   ],
   "source": [
    "#go down\n",
    "action = np.argwhere(env.action_text=='D')\n",
    "print(env.step(action))\n",
    "#then right\n",
    "action = np.argwhere(env.action_text=='R')\n",
    "print(env.step(action))\n",
    "env.print_physical(visible_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deterministic Environment vs Stochastic Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((2, 0), -1, False)\n",
      "((2, 1), -1, False)\n",
      "\n",
      "------------------------------------\n",
      "['F', 'x'] |['NA', 'NA'] |['NA', 'NA'] |\n",
      "------------------------------------\n",
      "['NA', 'NA'] |['NA', 'NA'] |['NA', 'NA'] |\n",
      "------------------------------------\n",
      "['F', 'x'] |['F', 'o'] |['NA', 'NA'] |"
     ]
    }
   ],
   "source": [
    "env = Gridworld(wind_p=0.5)\n",
    "#go down\n",
    "action = np.argwhere(env.action_text=='D')\n",
    "print(env.step(action))\n",
    "#then right\n",
    "action = np.argwhere(env.action_text=='R')\n",
    "print(env.step(action))\n",
    "env.print_physical(visible_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deterministic Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\pi(s) \\in \\mathcal{A}(s)$ for all $s \\in \\mathcal{S}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Grid\n",
      "\n",
      "----------\n",
      "0 |-5 |5 |\n",
      "----------\n",
      "0 |0 |0 |\n",
      "----------\n",
      "0 |0 |0 |\n",
      "\n",
      "Policy A: Reach Goal ASAP\n",
      "\n",
      "----------\n",
      "R |R |U |\n",
      "----------\n",
      "R |R |U |\n",
      "----------\n",
      "R |U |U |\n",
      "\n",
      "Policy B: Avoid Trap\n",
      "\n",
      "----------\n",
      "D |R |U |\n",
      "----------\n",
      "D |D |U |\n",
      "----------\n",
      "R |R |U |"
     ]
    }
   ],
   "source": [
    "#deterministic env\n",
    "env = Gridworld(wind_p=0.)\n",
    "#deterministic policy\n",
    "policy_a = {(0, 0): 3,\n",
    "          (0, 1): 3,\n",
    "          (0, 2): 0,\n",
    "          (1, 0): 3,\n",
    "          (1, 1): 3,\n",
    "          (1, 2): 0,\n",
    "          (2, 0): 3,\n",
    "          (2, 1): 0,\n",
    "          (2, 2): 0}\n",
    "policy_b = {(0, 0): 2,\n",
    "          (0, 1): 3,\n",
    "          (0, 2): 0,\n",
    "          (1, 0): 2,\n",
    "          (1, 1): 2,\n",
    "          (1, 2): 0,\n",
    "          (2, 0): 3,\n",
    "          (2, 1): 3,\n",
    "          (2, 2): 0}\n",
    "\n",
    "#peek\n",
    "print('Reward Grid')\n",
    "env.print_reward()\n",
    "print('\\n')\n",
    "a = MCAgent(env,policy_a)\n",
    "print('Policy A: Reach Goal ASAP')\n",
    "a.print_policy()\n",
    "print('\\n')\n",
    "print('Policy B: Avoid Trap')\n",
    "a.policy = policy_b\n",
    "a.print_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Policy - Epsilon Greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\pi(a|s) = \\mathbb{P}(A_t=a|S_t=s)$ for all $s \\in \\mathcal{S}$ and $a \\in \\mathcal{A}(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Assignment** Implement epsilon greedy in the `select_action` function of `agent.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preferred action for (0, 0) is 2\n",
      "At epsilon 0.5, we get:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<a list of 10 Patch objects>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "state = (0,0)\n",
    "print(f'Preferred action for {state} is {a.policy[state]}')\n",
    "print('At epsilon 0.5, we get:')\n",
    "actions = np.array([a.select_action(state,epsilon=0.5) for i in range(10000)])\n",
    "plt.hist(actions)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epsilon decaying \n",
    "a = MCAgent(env,policy_a,start_epsilon=0.9,end_epsilon=0.1,epsilon_decay=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f799288ec88>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps = np.array([a.get_epsilon(i) for i in range(100)])\n",
    "plt.plot(eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$v_\\pi(s) = \\sum_{a \\in \\mathcal{A}(s)}\\pi(a|s)\\sum_{s' \\in \\mathcal{S}, r\\in\\mathcal{R}}p(s',r|s,a)(r + \\gamma v_\\pi(s'))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Grid\n",
      "\n",
      "----------\n",
      "0 |-5 |5 |\n",
      "----------\n",
      "0 |0 |0 |\n",
      "----------\n",
      "0 |0 |0 |\n",
      "\n",
      "Policy: Reach Goal ASAP\n",
      "\n",
      "----------\n",
      "R |R |U |\n",
      "----------\n",
      "R |R |U |\n",
      "----------\n",
      "R |U |U |"
     ]
    }
   ],
   "source": [
    "#deterministic env\n",
    "env = Gridworld(wind_p=0.)\n",
    "policy = {(0, 0): 3,\n",
    "          (0, 1): 3,\n",
    "          (0, 2): 0,\n",
    "          (1, 0): 3,\n",
    "          (1, 1): 3,\n",
    "          (1, 2): 0,\n",
    "          (2, 0): 3,\n",
    "          (2, 1): 0,\n",
    "          (2, 2): 0}\n",
    "a = MCAgent(env,policy = policy, gamma = 1)\n",
    "print('Reward Grid')\n",
    "env.print_reward()\n",
    "print('\\n')\n",
    "print('Policy: Reach Goal ASAP')\n",
    "a.print_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Assignment** Implement `get_v` function of `agent.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------\n",
      "-2 |4 |-1 |\n",
      "---------------\n",
      "2 |3 |4 |\n",
      "---------------\n",
      "1 |2 |3 |"
     ]
    }
   ],
   "source": [
    "for state in env.state_space:\n",
    "    a.v[state] = a.get_v(start_state=state, epsilon=0.)\n",
    "a.print_v()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------\n",
      "-4.0 |4.0 |-1.0 |\n",
      "---------------\n",
      "-0.5 |1.0 |4.0 |\n",
      "---------------\n",
      "-1.2 |-0.5 |1.0 |"
     ]
    }
   ],
   "source": [
    "a.gamma = 0.5\n",
    "for state in env.state_space:\n",
    "    a.v[state] = a.get_v(start_state=state, epsilon=0.)\n",
    "a.print_v()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concept assigment** What are the best discount factor $\\gamma$ for each of the following environments?\n",
    "* [CartPole OpenAI](https://github.com/openai/gym/wiki/CartPole-v0) - An agent is a cart trying to balance a pole by going left or right. It gets +1 for each step the pole stays on and +0 when the pole falls over.\n",
    "* [CartPole Alternative](http://incompleteideas.net/book/the-book-2nd.html) - An agent is a cart trying to balance a pole by going left or right. It gets -1 if the pole falls over and +0 otherwise.\n",
    "* [Banana Collector](https://www.youtube.com/watch?v=heVMs3t9qSk) - An agent is a robot who collects bananas in a room. It gets +1 for yellow bananas and -1 for blue bananas. The time limit is 300 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (State-)Action Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$q_\\pi(s,a) = \\sum_{s' \\in \\mathcal{S}, r\\in\\mathcal{R}}p(s',r|s,a)(r + \\gamma\\sum_{a' \\in \\mathcal{A}(s')} \\pi(a'|s') q_\\pi(s',a'))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Assignment** Implement `get_q` function of `agent.py`. What's the difference between `get_q` and `get_v`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Grid\n",
      "\n",
      "----------\n",
      "0 |-5 |5 |\n",
      "----------\n",
      "0 |0 |0 |\n",
      "----------\n",
      "0 |0 |0 |\n",
      "\n",
      "Policy: Reach Goal ASAP\n",
      "\n",
      "----------\n",
      "R |R |U |\n",
      "----------\n",
      "R |R |U |\n",
      "----------\n",
      "R |U |U |\n",
      "Actions: ['U' 'L' 'D' 'R']\n",
      "(0, 0) [-3. -3.  1. -2.]\n",
      "(0, 1) [ 3. -3.  2.  4.]\n",
      "(0, 2) [-1. -2.  3. -1.]\n",
      "(1, 0) [-3.  1.  0.  2.]\n",
      "(1, 1) [-2.  1.  1.  3.]\n",
      "(1, 2) [4. 2. 2. 3.]\n",
      "(2, 0) [1. 0. 0. 1.]\n",
      "(2, 1) [2. 0. 1. 2.]\n",
      "(2, 2) [3. 1. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "print('Reward Grid')\n",
    "env.print_reward()\n",
    "print('\\n')\n",
    "print('Policy: Reach Goal ASAP')\n",
    "a.print_policy()\n",
    "\n",
    "a.gamma=1\n",
    "for state in env.state_space:\n",
    "    for action in env.action_space:\n",
    "        a.q[state][action] = a.get_q(state,action,epsilon=0.)\n",
    "\n",
    "print(f'\\nActions: {env.action_text}')\n",
    "for i in a.q: print(i,a.q[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing two policies a and b:\n",
    "\n",
    "$\\pi_a > \\pi_b$ if and only if $v_{\\pi_a}(s) > v_{\\pi_b}(s)$ for all $s \\in \\mathcal{S}$\n",
    "\n",
    "An optimal policy $\\pi^*$ is defined as:\n",
    "\n",
    "$\\pi^* > \\pi$ for all $\\pi$\n",
    "\n",
    "We can also find an optimal policy from action value function as choosing the action with the highest q-value out of any action in that state:\n",
    "\n",
    "$\\pi^*(s) = argmax_{a \\in \\mathcal{A}}q^*(s,a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy A: Reach Goal ASAP\n",
      "\n",
      "----------\n",
      "R |R |U |\n",
      "----------\n",
      "R |R |U |\n",
      "----------\n",
      "R |U |U |\n",
      "---------------\n",
      "-2.4 |4.0 |-1.0 |\n",
      "---------------\n",
      "1.3 |2.6 |4.0 |\n",
      "---------------\n",
      "0.2 |1.3 |2.6 |\n",
      "\n",
      "Policy B: Avoid Trap\n",
      "\n",
      "----------\n",
      "D |R |U |\n",
      "----------\n",
      "D |D |U |\n",
      "----------\n",
      "R |R |U |\n",
      "---------------\n",
      "-1.7 |4.0 |-1.0 |\n",
      "---------------\n",
      "-0.8 |0.2 |4.0 |\n",
      "---------------\n",
      "0.2 |1.3 |2.6 |"
     ]
    }
   ],
   "source": [
    "#deterministic env\n",
    "env = Gridworld(wind_p=0.)\n",
    "#deterministic policy\n",
    "policy_a = {(0, 0): 3,\n",
    "          (0, 1): 3,\n",
    "          (0, 2): 0,\n",
    "          (1, 0): 3,\n",
    "          (1, 1): 3,\n",
    "          (1, 2): 0,\n",
    "          (2, 0): 3,\n",
    "          (2, 1): 0,\n",
    "          (2, 2): 0}\n",
    "policy_b = {(0, 0): 2,\n",
    "          (0, 1): 3,\n",
    "          (0, 2): 0,\n",
    "          (1, 0): 2,\n",
    "          (1, 1): 2,\n",
    "          (1, 2): 0,\n",
    "          (2, 0): 3,\n",
    "          (2, 1): 3,\n",
    "          (2, 2): 0}\n",
    "\n",
    "a = MCAgent(env,policy_a)\n",
    "print('Policy A: Reach Goal ASAP')\n",
    "a.print_policy()\n",
    "for state in env.state_space:\n",
    "    a.v[state] = a.get_v(start_state=state, epsilon=0.)\n",
    "a.print_v()\n",
    "print('\\n')\n",
    "print('Policy B: Avoid Trap')\n",
    "a.policy = policy_b\n",
    "a.print_policy()\n",
    "for state in env.state_space:\n",
    "    a.v[state] = a.get_v(start_state=state, epsilon=0.)\n",
    "a.print_v()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Reinforcement Learning Problems - Monte Carlo Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main approaches in solving reinforcement learning: **model-based** and **model-free** approaches. A model-based approach assumes that we have some or full knowledge of how our environment works whereas a model-free approach relies on our agent to explore the environment without any prior knowledge. \n",
    "\n",
    "In this workshop, we will focus on model-free approaches which usually involves two steps: evalulating the state or action value function based on the agent's interactions with the environment also known as **prediction problem** and changing the agent's policy to be closer to an optimal policy also known as **control problem**.\n",
    "\n",
    "We start with the Monte Carlo Methods aka the trial-and-error-until-you-get-rich-or-broke methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Monte Carlo](img/monte_carlo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Grid\n",
      "\n",
      "----------\n",
      "0 |-5 |5 |\n",
      "----------\n",
      "0 |0 |0 |\n",
      "----------\n",
      "0 |0 |0 |\n",
      "\n",
      "Policy: Reach Goal ASAP\n",
      "\n",
      "----------\n",
      "R |R |U |\n",
      "----------\n",
      "R |R |U |\n",
      "----------\n",
      "R |U |U |"
     ]
    }
   ],
   "source": [
    "#stochastic environment\n",
    "env = Gridworld(wind_p=0.2)\n",
    "\n",
    "#initial policy\n",
    "policy = {(0, 0): 3,\n",
    "          (0, 1): 3,\n",
    "          (0, 2): 0,\n",
    "          (1, 0): 3,\n",
    "          (1, 1): 3,\n",
    "          (1, 2): 0,\n",
    "          (2, 0): 3,\n",
    "          (2, 1): 0,\n",
    "          (2, 2): 0}\n",
    "\n",
    "#stochastic agent - epsilon greedy with decays\n",
    "a = MCAgent(env, policy = policy, gamma = 0.9, \n",
    "            start_epsilon=0.9,end_epsilon=0.3,epsilon_decay=0.9)\n",
    "\n",
    "print('Reward Grid')\n",
    "env.print_reward()\n",
    "print('\\n')\n",
    "print('Policy: Reach Goal ASAP')\n",
    "a.print_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo State Value Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Monte Carlo State Value Prediction](img/mc_predict_v.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------\n",
      "-3.2 |2.5 |0 |\n",
      "---------------\n",
      "-1.1 |0.6 |2.8 |\n",
      "---------------\n",
      "-1.8 |-0.9 |0.8 |"
     ]
    }
   ],
   "source": [
    "a.mc_predict_v()\n",
    "a.print_v()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo Action Value Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Monte Carlo Action Value Prediction](img/mc_predict_q.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Actions: ['U' 'L' 'D' 'R']\n",
      "(0, 0) [-3.9453579  -4.15379883 -2.22871414 -3.27019098]\n",
      "(1, 0) [-3.76717188 -1.96185864 -2.41377348 -0.7582566 ]\n",
      "(2, 0) [-2.61606688 -3.1332547  -2.32319336 -1.52578252]\n",
      "(2, 1) [-0.69448481 -2.71918591 -2.34399761 -0.63118933]\n",
      "(2, 2) [ 1.29096544 -2.0527626  -0.59400853  0.17568285]\n",
      "(1, 2) [ 3.49633495 -0.83165808 -0.07926969  1.4554708 ]\n",
      "(1, 1) [-3.2971451  -1.98674747 -1.61504276  1.3794007 ]\n",
      "(0, 1) [ 1.24640547 -3.88688938 -0.69849022  3.4791249 ]\n",
      "(0, 2) [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "a.mc_predict_q(first_visit=False)\n",
    "print(f'\\nActions: {env.action_text}')\n",
    "for i in a.q: print(i,a.q[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Grid\n",
      "\n",
      "----------\n",
      "0 |-5 |5 |\n",
      "----------\n",
      "0 |0 |0 |\n",
      "----------\n",
      "0 |0 |0 |\n",
      "\n",
      "Policy: Reach Goal ASAP\n",
      "\n",
      "----------\n",
      "R |R |U |\n",
      "----------\n",
      "R |R |U |\n",
      "----------\n",
      "R |U |U |"
     ]
    }
   ],
   "source": [
    "#stochastic environment\n",
    "env = Gridworld(wind_p=0.2)\n",
    "\n",
    "#initial policy\n",
    "policy = {(0, 0): 3,\n",
    "          (0, 1): 3,\n",
    "          (0, 2): 0,\n",
    "          (1, 0): 3,\n",
    "          (1, 1): 3,\n",
    "          (1, 2): 0,\n",
    "          (2, 0): 3,\n",
    "          (2, 1): 0,\n",
    "          (2, 2): 0}\n",
    "\n",
    "#stochastic agent - epsilon greedy with decays\n",
    "a = MCAgent(env, policy = policy, gamma = 0.9, \n",
    "            start_epsilon=0.9,end_epsilon=0.3,epsilon_decay=0.9)\n",
    "\n",
    "print('Reward Grid')\n",
    "env.print_reward()\n",
    "print('\\n')\n",
    "print('Policy: Reach Goal ASAP')\n",
    "a.print_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All-visit Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Assignment** Implement `mc_control_q` function of `agent.py` using either all-visit or first-visit Monte Carlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------\n",
      "D |R |U |\n",
      "----------\n",
      "R |R |U |\n",
      "----------\n",
      "D |R |U |\n",
      "Actions: ['U' 'L' 'D' 'R']\n",
      "(0, 0) [-3.71145637 -3.73476388 -1.93291734 -3.08592739]\n",
      "(0, 1) [ 0.81986429 -3.42699734 -0.77561971  3.49921695]\n",
      "(1, 1) [-1.45736971 -3.47766418 -1.66144641  1.42421693]\n",
      "(1, 2) [ 3.56672025 -1.12376792 -1.11773791  1.097635  ]\n",
      "(1, 0) [-4.5176349  -2.87546744 -2.50694923 -0.79525219]\n",
      "(2, 0) [-3.632104   -2.9318347  -0.8146     -1.15123686]\n",
      "(2, 1) [-0.48045889 -4.2053248  -0.581158    0.6218    ]\n",
      "(2, 2) [ 0.99130823 -3.83815628  0.773      -0.53163229]\n",
      "(0, 2) [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#reset\n",
    "a.policy = policy\n",
    "a.q = defaultdict(lambda: np.zeros(a.n_action))\n",
    "a.n_q = defaultdict(lambda: np.zeros(a.n_action))\n",
    "\n",
    "a.mc_control_q(n_episode = 1000,first_visit=False)\n",
    "a.print_policy()\n",
    "print(f'\\nActions: {env.action_text}')\n",
    "for i in a.q: print(i,a.q[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First-visit Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------\n",
      "D |R |U |\n",
      "----------\n",
      "R |R |U |\n",
      "----------\n",
      "R |R |U |\n",
      "Actions: ['U' 'L' 'D' 'R']\n",
      "(0, 0) [-3.67623761 -3.61003645 -2.59279023 -3.62205406]\n",
      "(1, 0) [-3.2375625  -2.49771844 -5.34846987 -0.78263739]\n",
      "(2, 1) [-0.52324342 -4.47573337 -1.1470465  -0.29400651]\n",
      "(1, 1) [-2.92171618 -2.51850567 -1.56386673  1.32527245]\n",
      "(1, 2) [ 3.38108387 -1.70764441 -0.44166736  0.86341471]\n",
      "(2, 0) [-2.56750386 -5.33626478 -5.67093969 -1.94096582]\n",
      "(2, 2) [ 1.28318157 -1.41581075 -0.0101933  -0.94318053]\n",
      "(0, 1) [ 0.88357398 -3.09952131 -0.27923792  3.03531876]\n",
      "(0, 2) [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#reset\n",
    "a.policy = policy\n",
    "a.q = defaultdict(lambda: np.zeros(a.n_action))\n",
    "a.n_q = defaultdict(lambda: np.zeros(a.n_action))\n",
    "\n",
    "a.mc_control_q(n_episode = 1000,first_visit=True)\n",
    "a.print_policy()\n",
    "print(f'\\nActions: {env.action_text}')\n",
    "for i in a.q: print(i,a.q[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy within The Limit of Exploration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Greedy within The Limit of Exploration](img/mc_control_glie.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Assignment** Implement `mc_control_glie` function of `agent.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------\n",
      "D |R |U |\n",
      "----------\n",
      "R |R |U |\n",
      "----------\n",
      "R |R |U |\n",
      "Actions: ['U' 'L' 'D' 'R']\n",
      "(0, 0) [-2.93171411 -2.90030702 -1.92682242 -3.02698423]\n",
      "(0, 1) [ 1.16250065 -4.87394682 -1.06677532  3.46019962]\n",
      "(0, 2) [0. 0. 0. 0.]\n",
      "(1, 0) [-3.02771505 -2.29929109 -2.5696213  -0.69499358]\n",
      "(1, 1) [-3.42660959 -1.8680519  -1.34479693  1.55381323]\n",
      "(1, 2) [ 3.53300077 -0.64473196 -0.47259996  1.15403313]\n",
      "(2, 0) [-3.90213486 -2.56709707 -2.2689575  -1.36999764]\n",
      "(2, 1) [-0.51502287 -2.49376365 -1.68546978 -0.07480667]\n",
      "(2, 2) [ 1.41895696 -0.90372439 -0.12253809 -0.04078553]\n"
     ]
    }
   ],
   "source": [
    "#reset\n",
    "a.policy = policy\n",
    "a.q = defaultdict(lambda: np.zeros(a.n_action))\n",
    "a.n_q = defaultdict(lambda: np.zeros(a.n_action))\n",
    "\n",
    "a.mc_control_glie(n_episode = 1000)\n",
    "a.print_policy()\n",
    "print(f'\\nActions: {env.action_text}')\n",
    "for i in a.q: print(i,a.q[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GLIE with Constant Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GLIE with constant learning rate](img/mc_control_glie_constant.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------\n",
      "D |R |U |\n",
      "----------\n",
      "R |R |U |\n",
      "----------\n",
      "R |R |U |\n",
      "Actions: ['U' 'L' 'D' 'R']\n",
      "(0, 0) [-3.0093252  -3.53621447 -1.7453115  -2.49533629]\n",
      "(0, 1) [ 0.9782357  -3.49175286 -0.51669182  3.89722936]\n",
      "(0, 2) [0. 0. 0. 0.]\n",
      "(1, 0) [-3.39225087 -2.20413277 -2.17974173 -1.0316493 ]\n",
      "(1, 1) [-3.75088582 -2.12919057 -1.65385739  1.54396361]\n",
      "(1, 2) [ 3.52319569 -0.50105682  0.82994111  1.44489271]\n",
      "(2, 0) [-3.44630153 -3.35845762 -2.99478041 -0.75427243]\n",
      "(2, 1) [-1.13678223 -2.76713091 -2.01246241  0.86117625]\n",
      "(2, 2) [ 2.33529368 -2.1589869  -0.05309148 -0.73294849]\n"
     ]
    }
   ],
   "source": [
    "#reset\n",
    "a.policy = policy\n",
    "a.q = defaultdict(lambda: np.zeros(a.n_action))\n",
    "a.n_q = defaultdict(lambda: np.zeros(a.n_action))\n",
    "\n",
    "a.mc_control_glie(n_episode = 1000, lr=0.1)\n",
    "a.print_policy()\n",
    "print(f'\\nActions: {env.action_text}')\n",
    "for i in a.q: print(i,a.q[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What are some other ways of solving reinforcement learning problems? How are they better or worse than Monte Carlo methods e.g. performance, data requirements, etc.?\n",
    "* Play around with Gridworld. Tweak these variables and see what happens:\n",
    "    * Wing probability\n",
    "    * Move rewards\n",
    "    * Discount factor\n",
    "    * Epsilon and how to decay it (or not)\n",
    "* Solve at least one of the following OpenAI gym environments with discrete states and actions:\n",
    "    * FrozenLake-v0\n",
    "    * Taxi-v2\n",
    "    * Blackjack-v0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
