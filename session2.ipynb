{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2: Monte Carlo Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment only if you're running from google colab\n",
    "# !git clone https://github.com/Datatouille/rl-workshop\n",
    "# !mv rl-workshop/* .\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "#cross check with our solutions once you finish\n",
    "from solutions.agents import MCAgent\n",
    "from solutions.environments import Gridworld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill in The Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from collections import defaultdict\n",
    "# import sys\n",
    "\n",
    "# \"\"\"\n",
    "# Coding assignment order:\n",
    "# 1. get_v\n",
    "# 2. get_q\n",
    "# 3. mc_control_q\n",
    "# 4. mc_control_glie\n",
    "# \"\"\"\n",
    "\n",
    "# class MCAgent:\n",
    "#     def __init__(self, env, policy, gamma = 0.9, \n",
    "#                  start_epsilon = 0.9, end_epsilon = 0.1, epsilon_decay = 0.9):\n",
    "#         self.env = env\n",
    "#         self.n_action = len(self.env.action_space)\n",
    "#         self.policy = policy\n",
    "#         self.gamma = gamma\n",
    "#         self.v = dict.fromkeys(self.env.state_space,0)\n",
    "#         self.n_v = dict.fromkeys(self.env.state_space,0)\n",
    "#         self.q = defaultdict(lambda: np.zeros(self.n_action))\n",
    "#         self.n_q = defaultdict(lambda: np.zeros(self.n_action))\n",
    "#         self.start_epsilon = start_epsilon\n",
    "#         self.end_epsilon = end_epsilon\n",
    "#         self.epsilon_decay = epsilon_decay\n",
    "#     def get_epsilon(self,n_episode):\n",
    "#         epsilon = max(self.start_epsilon * (self.epsilon_decay**n_episode),self.end_epsilon)\n",
    "#         return(epsilon)\n",
    "#     def get_v(self,start_state,epsilon = 0.):\n",
    "#         episode = self.run_episode(start_state,epsilon)\n",
    "#         \"\"\"\n",
    "#         Write the code to calculate the state value function of a state \n",
    "#         given a deterministic policy.\n",
    "#         \"\"\"\n",
    "#         v=0\n",
    "        \n",
    "#         return(v)\n",
    "#     def get_q(self, start_state, first_action, epsilon=0.):\n",
    "#         episode = self.run_episode(start_state,epsilon,first_action)\n",
    "#         \"\"\"\n",
    "#         Write the code to calculate the action function of a state \n",
    "#         given a deterministic policy.\n",
    "#         \"\"\"\n",
    "#         q=0\n",
    "#         return(q)\n",
    "#     def select_action(self,state,epsilon):\n",
    "#         \"\"\"\n",
    "#         Currently the agent only selects a random action.\n",
    "#         Write the code to make the agent perform \n",
    "#         according to an epsilon-greedy policy.\n",
    "#         \"\"\"\n",
    "#         probs = np.ones(self.n_action) * (1 / self.n_action)\n",
    "#         action = np.random.choice(np.arange(self.n_action),p=probs)\n",
    "#         return(action)\n",
    "#     def print_policy(self):\n",
    "#         for i in range(self.env.sz[0]):\n",
    "#             print('\\n----------')\n",
    "#             for j in range(self.env.sz[1]):\n",
    "#                 p=self.policy[(i,j)]\n",
    "#                 out = self.env.action_text[p]\n",
    "#                 print(f'{out} |',end='')\n",
    "#     def print_v(self, decimal = 1):\n",
    "#         for i in range(self.env.sz[0]):\n",
    "#             print('\\n---------------')\n",
    "#             for j in range(self.env.sz[1]):\n",
    "#                 out=np.round(self.v[(i,j)],decimal)\n",
    "#                 print(f'{out} |',end='')\n",
    "#     def run_episode(self, start, epsilon, first_action = None):\n",
    "#         result = []\n",
    "#         state = self.env.reset(start)\n",
    "#         #dictate first action to iterate q\n",
    "#         if first_action is not None:\n",
    "#             action = first_action\n",
    "#             next_state,reward,done = self.env.step(action)\n",
    "#             result.append((state,action,reward,next_state,done))\n",
    "#             state = next_state\n",
    "#             if done: return(result)\n",
    "#         while True:\n",
    "#             action = self.select_action(state,epsilon)\n",
    "#             next_state,reward,done = self.env.step(action)\n",
    "#             result.append((state,action,reward,next_state,done))\n",
    "#             state = next_state\n",
    "#             if done: break\n",
    "#         return(result)\n",
    "#     def update_policy_q(self):\n",
    "#         for state in self.env.state_space:\n",
    "#             self.policy[state] = np.argmax(self.q[state])\n",
    "#     def mc_predict_v(self,n_episode=10000,first_visit=True):\n",
    "#         for t in range(n_episode):\n",
    "#             traversed = []\n",
    "#             e = self.get_epsilon(t)\n",
    "#             transitions = self.run_episode(self.env.start,e)\n",
    "#             states,actions,rewards,next_states,dones = zip(*transitions)\n",
    "#             for i in range(len(transitions)):\n",
    "#                 if first_visit and (states[i] not in traversed):\n",
    "#                     traversed.append(states[i])\n",
    "#                     self.n_v[states[i]]+=1\n",
    "#                     discounts = np.array([self.gamma**j for j in range(len(transitions)+1)])\n",
    "#                     self.v[states[i]]+= sum(rewards[i:]*discounts[:-(1+i)])\n",
    "#         for state in self.env.state_space:\n",
    "#             if state != self.env.goal:\n",
    "#                 self.v[state] = self.v[state] / self.n_v[state]\n",
    "#             else:\n",
    "#                 self.v[state] = 0\n",
    "    \n",
    "#     def mc_predict_q(self,n_episode=10000,first_visit=True):\n",
    "#         for t in range(n_episode):\n",
    "#             traversed = []\n",
    "#             e = self.get_epsilon(t)\n",
    "#             transitions = self.run_episode(self.env.start,e)\n",
    "#             states,actions,rewards,next_states,dones = zip(*transitions)\n",
    "#             for i in range(len(transitions)):\n",
    "#                 if first_visit and ((states[i],actions[i]) not in traversed):\n",
    "#                     traversed.append((states[i],actions[i]))\n",
    "#                     self.n_q[states[i]][actions[i]]+=1\n",
    "#                     discounts = np.array([self.gamma**j for j in range(len(transitions)+1)])\n",
    "#                     self.q[states[i]][actions[i]]+= sum(rewards[i:]*discounts[:-(1+i)])\n",
    "#                 elif not first_visit:\n",
    "#                     self.n_q[states[i]][actions[i]]+=1\n",
    "#                     discounts = np.array([self.gamma**j for j in range(len(transitions)+1)])\n",
    "#                     self.q[states[i]][actions[i]]+= sum(rewards[i:]*discounts[:-(1+i)])\n",
    "\n",
    "#         #print(self.q,self.n_q)\n",
    "#         for state in self.env.state_space:\n",
    "#             for action in range(self.n_action):\n",
    "#                 if state != self.env.goal:\n",
    "#                     self.q[state][action] = self.q[state][action] / self.n_q[state][action]\n",
    "#                 else:\n",
    "#                     self.q[state][action] = 0\n",
    "        \n",
    "#     def mc_control_q(self,n_episode=10000,first_visit=True):\n",
    "#         \"\"\"\n",
    "#         Write the code to perform Monte Carlo Control\n",
    "#         Hint: You just need to do prediction then update the policy\n",
    "#         \"\"\"\n",
    "#         pass\n",
    "        \n",
    "#     def mc_control_glie(self,n_episode=10000,first_visit=True,lr=0.):\n",
    "#         \"\"\"\n",
    "#         Taking hints from the mc_predict_q and mc_control_q methods, write the code to\n",
    "#         perform GLIE Monte Carlo control.\n",
    "#         \"\"\"\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RL Framework](img/rl_framework.png)\n",
    "Source: [Sutton and Barto](https://cdn.preterhuman.net/texts/science_and_technology/artificial_intelligence/Reinforcement%20Learning%20%20An%20Introduction%20-%20Richard%20S.%20Sutton%20,%20Andrew%20G.%20Barto.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Reinforcement Learning Problems - Monte Carlo Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main approaches in solving reinforcement learning: **model-based** and **model-free** approaches. A model-based approach assumes that we have some or full knowledge of how our environment works whereas a model-free approach relies on our agent to explore the environment without any prior knowledge. \n",
    "\n",
    "In this workshop, we will focus on model-free approaches which usually involves two steps: evalulating the state or action value function based on the agent's interactions with the environment also known as **prediction problem** and changing the agent's policy to be closer to an optimal policy also known as **control problem**.\n",
    "\n",
    "We start with the Monte Carlo Methods aka the trial-and-error-until-you-get-rich-or-broke methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Monte Carlo](img/monte_carlo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Grid\n",
      "\n",
      "----------\n",
      "0 |-5 |5 |\n",
      "----------\n",
      "0 |0 |0 |\n",
      "----------\n",
      "0 |0 |0 |\n",
      "\n",
      "Policy: Reach Goal ASAP\n",
      "\n",
      "----------\n",
      "R |R |U |\n",
      "----------\n",
      "R |R |U |\n",
      "----------\n",
      "R |U |U |"
     ]
    }
   ],
   "source": [
    "#stochastic environment\n",
    "env = Gridworld(wind_p=0.2)\n",
    "\n",
    "#initial policy\n",
    "policy = {(0, 0): 3,\n",
    "          (0, 1): 3,\n",
    "          (0, 2): 0,\n",
    "          (1, 0): 3,\n",
    "          (1, 1): 3,\n",
    "          (1, 2): 0,\n",
    "          (2, 0): 3,\n",
    "          (2, 1): 0,\n",
    "          (2, 2): 0}\n",
    "\n",
    "#stochastic agent - epsilon greedy with decays\n",
    "a = MCAgent(env, policy = policy, gamma = 0.9, \n",
    "            start_epsilon=0.9,end_epsilon=0.3,epsilon_decay=0.9)\n",
    "\n",
    "print('Reward Grid')\n",
    "env.print_reward()\n",
    "print('\\n')\n",
    "print('Policy: Reach Goal ASAP')\n",
    "a.print_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo State Value Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Monte Carlo State Value Prediction](img/mc_predict_v.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------\n",
      "-3.3 |2.4 |0 |\n",
      "---------------\n",
      "-1.3 |0.5 |2.7 |\n",
      "---------------\n",
      "-1.9 |-0.8 |1.1 |"
     ]
    }
   ],
   "source": [
    "a.mc_predict_v()\n",
    "a.print_v()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo Action Value Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Monte Carlo Action Value Prediction](img/mc_predict_q.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Actions: ['U' 'L' 'D' 'R']\n",
      "(0, 0) [-3.87520234 -3.95176709 -2.12485939 -3.27277443]\n",
      "(0, 1) [ 1.27172355 -3.90926609 -0.65511419  3.47127782]\n",
      "(1, 1) [-3.28431787 -2.37673699 -2.00285685  1.32309232]\n",
      "(1, 2) [ 3.43092627 -0.82052425 -0.12168492  1.7112537 ]\n",
      "(2, 2) [ 1.25205736 -1.42568763 -0.18455822  0.27898255]\n",
      "(1, 0) [-3.56882138 -1.81379909 -2.92298905 -0.65771682]\n",
      "(2, 0) [-2.3871678  -2.29697105 -2.82679549 -1.88678102]\n",
      "(2, 1) [-0.9921875  -3.14429309 -2.11968231 -0.70891247]\n",
      "(0, 2) [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "a.mc_predict_q(first_visit=False)\n",
    "print(f'\\nActions: {env.action_text}')\n",
    "for i in a.q: print(i,a.q[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Grid\n",
      "\n",
      "----------\n",
      "0 |-5 |5 |\n",
      "----------\n",
      "0 |0 |0 |\n",
      "----------\n",
      "0 |0 |0 |\n",
      "\n",
      "Policy: Reach Goal ASAP\n",
      "\n",
      "----------\n",
      "R |R |U |\n",
      "----------\n",
      "R |R |U |\n",
      "----------\n",
      "R |U |U |"
     ]
    }
   ],
   "source": [
    "#stochastic environment\n",
    "env = Gridworld(wind_p=0.2)\n",
    "\n",
    "#initial policy\n",
    "policy = {(0, 0): 3,\n",
    "          (0, 1): 3,\n",
    "          (0, 2): 0,\n",
    "          (1, 0): 3,\n",
    "          (1, 1): 3,\n",
    "          (1, 2): 0,\n",
    "          (2, 0): 3,\n",
    "          (2, 1): 0,\n",
    "          (2, 2): 0}\n",
    "\n",
    "#stochastic agent - epsilon greedy with decays\n",
    "a = MCAgent(env, policy = policy, gamma = 0.9, \n",
    "            start_epsilon=0.9,end_epsilon=0.3,epsilon_decay=0.9)\n",
    "\n",
    "print('Reward Grid')\n",
    "env.print_reward()\n",
    "print('\\n')\n",
    "print('Policy: Reach Goal ASAP')\n",
    "a.print_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All-visit Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Assignment** Implement `mc_control_q` function of `agent.py` using either all-visit or first-visit Monte Carlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------\n",
      "D |R |U |\n",
      "----------\n",
      "R |R |U |\n",
      "----------\n",
      "U |R |U |\n",
      "Actions: ['U' 'L' 'D' 'R']\n",
      "(0, 0) [-3.3648072  -3.00271641 -1.97255159 -3.06164953]\n",
      "(1, 0) [-3.10504148 -2.53907901 -2.81174181 -0.70849412]\n",
      "(1, 1) [-3.12555336 -2.11457103 -2.33230348  1.43066392]\n",
      "(2, 1) [-0.98389343 -2.76532118 -2.06755475 -0.39588622]\n",
      "(2, 2) [ 1.31560825 -0.7496164  -1.24977903 -0.529901  ]\n",
      "(1, 2) [ 3.54235505 -1.12257708 -0.34551398  1.94557868]\n",
      "(2, 0) [-1.45549503 -3.07723619 -3.25416013 -1.84380456]\n",
      "(0, 1) [ 0.54128492 -3.12071991 -0.52837533  3.31185729]\n",
      "(0, 2) [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#reset\n",
    "a.policy = policy\n",
    "a.q = defaultdict(lambda: np.zeros(a.n_action))\n",
    "a.n_q = defaultdict(lambda: np.zeros(a.n_action))\n",
    "\n",
    "a.mc_control_q(n_episode = 1000,first_visit=False)\n",
    "a.print_policy()\n",
    "print(f'\\nActions: {env.action_text}')\n",
    "for i in a.q: print(i,a.q[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First-visit Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------\n",
      "D |R |U |\n",
      "----------\n",
      "R |R |U |\n",
      "----------\n",
      "R |R |U |\n",
      "Actions: ['U' 'L' 'D' 'R']\n",
      "(0, 0) [-3.33090506 -3.35264948 -2.37680824 -3.53154985]\n",
      "(0, 1) [ 1.12557948 -3.59881149 -0.07597566  3.53821045]\n",
      "(1, 1) [-3.25784061 -3.01504744 -1.89861301  1.41272891]\n",
      "(1, 2) [ 3.42888709 -0.90041875  0.08545299  1.43636991]\n",
      "(2, 2) [ 1.29705437 -1.17685305 -0.18071458 -0.14363666]\n",
      "(1, 0) [-3.62809754 -2.37393907 -5.09054853 -0.82914496]\n",
      "(2, 1) [-0.47162352 -4.06410253 -2.06969252 -0.31899824]\n",
      "(2, 0) [-2.21098816 -5.8956048  -5.46757323 -1.58178655]\n",
      "(0, 2) [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#reset\n",
    "a.policy = policy\n",
    "a.q = defaultdict(lambda: np.zeros(a.n_action))\n",
    "a.n_q = defaultdict(lambda: np.zeros(a.n_action))\n",
    "\n",
    "a.mc_control_q(n_episode = 1000,first_visit=True)\n",
    "a.print_policy()\n",
    "print(f'\\nActions: {env.action_text}')\n",
    "for i in a.q: print(i,a.q[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy within The Limit of Exploration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Greedy within The Limit of Exploration](img/mc_control_glie.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Assignment** Implement `mc_control_glie` function of `agent.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------\n",
      "D |R |U |\n",
      "----------\n",
      "R |R |U |\n",
      "----------\n",
      "R |R |U |\n",
      "Actions: ['U' 'L' 'D' 'R']\n",
      "(0, 0) [-2.72464852 -2.91183396 -1.92453351 -3.46585419]\n",
      "(0, 1) [ 1.66495903 -2.69893544 -1.10813084  3.35280643]\n",
      "(0, 2) [0. 0. 0. 0.]\n",
      "(1, 0) [-2.56053522 -1.84431052 -2.26094775 -0.59527436]\n",
      "(1, 1) [-3.1760948  -2.15118815 -1.03895928  1.47737819]\n",
      "(1, 2) [ 3.50871626 -1.05467604 -0.52847885  1.64152426]\n",
      "(2, 0) [-1.78898768 -2.36953902 -2.04681068 -1.04088752]\n",
      "(2, 1) [-7.81846524e-01 -2.59569786e+00 -9.75411343e-01 -1.15386603e-03]\n",
      "(2, 2) [ 1.32325351 -1.00171223  0.08261916 -1.01438424]\n"
     ]
    }
   ],
   "source": [
    "#reset\n",
    "a.policy = policy\n",
    "a.q = defaultdict(lambda: np.zeros(a.n_action))\n",
    "a.n_q = defaultdict(lambda: np.zeros(a.n_action))\n",
    "\n",
    "a.mc_control_glie(n_episode = 1000)\n",
    "a.print_policy()\n",
    "print(f'\\nActions: {env.action_text}')\n",
    "for i in a.q: print(i,a.q[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GLIE with Constant Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GLIE with constant learning rate](img/mc_control_glie_constant.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------\n",
      "D |R |U |\n",
      "----------\n",
      "R |R |U |\n",
      "----------\n",
      "R |U |U |\n",
      "Actions: ['U' 'L' 'D' 'R']\n",
      "(0, 0) [-2.49654223 -3.99477908 -1.64247075 -3.50054956]\n",
      "(0, 1) [ 0.4485978  -3.32652596  0.26543862  3.79139635]\n",
      "(0, 2) [0. 0. 0. 0.]\n",
      "(1, 0) [-3.65128977 -1.98788925 -2.22685661 -0.05216226]\n",
      "(1, 1) [-3.30011964 -1.41961964 -1.09464179  1.83995133]\n",
      "(1, 2) [ 3.99477322 -0.35830135  0.07677462  2.29888339]\n",
      "(2, 0) [-2.96852594 -2.31099869 -2.08115679 -0.85586161]\n",
      "(2, 1) [ 0.1332173  -1.27800182 -1.37604754 -0.33777095]\n",
      "(2, 2) [ 1.47662675 -0.64081251 -0.30612377 -0.67713795]\n"
     ]
    }
   ],
   "source": [
    "#reset\n",
    "a.policy = policy\n",
    "a.q = defaultdict(lambda: np.zeros(a.n_action))\n",
    "a.n_q = defaultdict(lambda: np.zeros(a.n_action))\n",
    "\n",
    "a.mc_control_glie(n_episode = 1000, lr=0.1)\n",
    "a.print_policy()\n",
    "print(f'\\nActions: {env.action_text}')\n",
    "for i in a.q: print(i,a.q[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What are some other ways of solving reinforcement learning problems? How are they better or worse than Monte Carlo methods e.g. performance, data requirements, etc.?\n",
    "* Solve at least one of the following OpenAI gym environments with discrete states and actions:\n",
    "    * FrozenLake-v0\n",
    "    * Taxi-v2\n",
    "    * Blackjack-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
